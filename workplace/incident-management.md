# Incident Management â€” äº‹æ•…è™•ç†è‹±æ–‡å¤§å…¨

## Core Vocabulary
- severity åš´é‡åº¦ï¼šsev-1/2/3, P0/P1
- impact å½±éŸ¿ï¼šblast radius, affected users
- detection åµæ¸¬ï¼šalerts, monitoring
- mitigation ç·©è§£ï¼šworkaround, throttle, failover, rollback
- timeline æ™‚åºï¼šincident timeline, TTD/MTTR
- communication æºé€šï¼šstatus update, stakeholder brief, status page
- root cause æ ¹å› ï¼šRCA, contributing factors
- action items è¡Œå‹•é …ï¼šcorrective actions, follow-ups
- runbook æ“ä½œæ‰‹å†Šï¼šSOP, playbook

## Useful Patterns
- åˆæ¬¡å…¬å‘Šï¼ˆå…§éƒ¨ï¼‰ï¼š
  - We're investigating a sev-1 impacting [service]; symptoms: [â€¦]. Next update at [time].
- ç‹€æ…‹é æ›´æ–°ï¼ˆå°å¤–ï¼‰ï¼š
  - Identified the issue; implementing mitigation. Next update in 30 minutes.
- äº¤æ¥/è¼ªç­ï¼š
  - Handover: current state, active mitigations, owners per stream.
- å›å¾©èˆ‡æ”¶å°¾ï¼š
  - Service has recovered; we're monitoring for 1 hour before closure.
- äº‹å¾Œæª¢è¨ï¼ˆPostmortemï¼‰éª¨æ¶ï¼š
  - Summary, Impact, Timeline, Root Cause, What Went Well, What Went Wrong, Action Items, Preventive Measures.

## Context Examples
- Slack æˆ°æƒ…å®¤ï¼š
  - "Requesting DB on-call to check replication lag; web errors at 8% 5xx."
- å®¢æˆ¶ä¿¡ï¼ˆSev-1ï¼‰ï¼š
  - "We apologize for the disruption between 09:10â€“09:55 UTC; root cause was a config regression. We're adding a safeguard and improving tests."
- ç®¡ç†å±¤ç°¡å ±ï¼š
  - "Impact limited to APAC; MTTR 42m; no data loss; actions on slide 5."

## Dialogue Examples â€” å¯¦éš›å°è©±ç¯„ä¾‹

### ğŸš¨ äº‹æ•…æˆ°æƒ…å®¤å°è©±
**æƒ…å¢ƒ**ï¼šç·Šæ€¥äº‹æ•…è™•ç†ä¸­çš„åœ˜éšŠå”èª¿

**Incident Commander**: We've got a Sev-1 affecting 20% of users. Payment processing is down. Current status?

**Engineer (Mike)**: Database connections are maxed out. I'm checking for a connection leak in the payment service.

**SRE (Lisa)**: I see a spike starting 10 minutes ago. Could be related to the deployment at 2:15.

**IC**: Let's roll back that deployment first to mitigate. How long to roll back?

**DevOps (Sam)**: 5 minutes to rollback, assuming no DB schema changes. Checking now... we're clear.

**IC**: Execute the rollback. I'll update the status page that we've identified the issue and are applying a fix.

### ğŸ“ å®¢æˆ¶æºé€š
**æƒ…å¢ƒ**ï¼šå‘å®¢æˆ¶èªªæ˜æœå‹™ä¸­æ–·æƒ…æ³

**Support Lead**: We're getting calls about checkout failures. What should I tell customers?

**IC**: We've identified the issue and are applying a fix. ETR is 15 minutes. Tell them to retry after 2:45 PM.

**Support Lead**: Got it. Should I proactively reach out to enterprise customers?

**IC**: Yes, and offer to extend their trial period by one day for the inconvenience.

**Support Lead**: I'll draft the communication now and send it out.

### ğŸ“ˆ äº‹å¾Œæª¢è¨æœƒè­°
**æƒ…å¢ƒ**ï¼šäº‹æ•…å¾Œçš„åœ˜éšŠæª¢è¨å’Œæ”¹é€²è¨è«–

**Facilitator**: Let's walk through the timeline. What went well?

**Engineer**: Detection was fastâ€”alerts fired within 2 minutes of the issue starting.

**SRE**: Communication was clear. Everyone knew their role and responsibilities.

**Facilitator**: What could we improve?

**DevOps**: We need better rollback automation. Manual steps took too long and increased MTTR.

**Manager**: Agreed. Let's prioritize automated rollbacks for next sprint.

**Engineer**: Also, we should add connection pool monitoring to catch this sooner.

### ğŸ”„ äº‹æ•…ç‹€æ…‹æ›´æ–°
**æƒ…å¢ƒ**ï¼šå‘ç®¡ç†å±¤å’Œåˆ©å®³é—œä¿‚äººå ±å‘Šé€²å±•

**IC**: Update for leadership: We've mitigated the payment issue by rolling back. Error rate dropped from 15% to 0.2%.

**VP Engineering**: What's the root cause?

**IC**: Connection pool exhaustion due to a configuration change. We're analyzing why the change passed testing.

**VP Engineering**: Customer impact?

**IC**: Approximately 500 customers experienced checkout failures. Customer success is reaching out with credits.

**VP Engineering**: Timeline for permanent fix?

**IC**: Full analysis tomorrow, fix deployment by end of week.

### ğŸ“Š äº‹æ•…æŒ‡æ¨™è¨è«–
**æƒ…å¢ƒ**ï¼šæª¢è¨äº‹æ•…éŸ¿æ‡‰æŒ‡æ¨™å’Œæ”¹é€²ç›®æ¨™

**SRE Manager**: Let's review our SLIs for this incident. MTTD was 2 minutes, MTTR was 18 minutes.

**Engineer**: MTTD was good, but MTTR could be better. The rollback process took 12 minutes.

**DevOps Lead**: We can automate that. Target should be 5-minute rollbacks.

**SRE Manager**: What about customer communication?

**Support Manager**: First customer notification went out 8 minutes after detection. We can improve that to 5 minutes.

**SRE Manager**: Let's set those as our targets for next quarter.

### ğŸ“ Postmortem æ’°å¯«
**æƒ…å¢ƒ**ï¼šåœ˜éšŠå”ä½œæ’°å¯«äº‹æ•…å ±å‘Š

**Tech Writer**: I'm drafting the postmortem. Can someone explain the technical root cause?

**Senior Engineer**: The connection pool size was reduced from 100 to 20 in the config change, but we didn't update the load test parameters.

**Tech Writer**: Why didn't the staging tests catch this?

**QA Lead**: Staging only has 10% of production traffic. The issue only manifests at full load.

**Tech Writer**: What's the action item?

**Senior Engineer**: Update load testing to use production-scale traffic patterns.

### ğŸ› ï¸ é é˜²æªæ–½è¨è«–
**æƒ…å¢ƒ**ï¼šåˆ¶å®šé˜²æ­¢é¡ä¼¼äº‹æ•…å†æ¬¡ç™¼ç”Ÿçš„ç­–ç•¥

**Engineering Manager**: How do we prevent this type of issue in the future?

**SRE**: We need connection pool monitoring with alerts before we hit 80% utilization.

**DevOps**: Also, configuration changes should require performance review, not just functional review.

**QA**: We can add synthetic transactions that simulate peak load continuously.

**Engineering Manager**: Timeline for implementing these?

**SRE**: Monitoring can be done this week. Load testing improvements need 3 weeks.

**Engineering Manager**: Let's track these in our reliability backlog.

## Mini Drills
- å¯« 4 å¥ç‹€æ…‹é æ›´æ–°ï¼ˆInvestigating/Identified/Monitoring/Resolvedï¼‰ã€‚
- ç”¨ 5 å¥è©±å¯« Postmortem æ‘˜è¦ï¼ˆå«å½±éŸ¿èˆ‡å…©é …è¡Œå‹•ï¼‰ã€‚

## Quick Reference â€” ä¸­è‹±é›™èªå¡
- We're investigating a sev-1 incident. æˆ‘å€‘æ­£åœ¨èª¿æŸ¥ä¸€å€‹æœ€åš´é‡ç­‰ç´šäº‹ä»¶ã€‚
- Issue identified; applying mitigation. å•é¡Œå·²æ‰¾å‡ºï¼›æ­£åœ¨ç·©è§£ã€‚
- Service has recovered; we're monitoring. æœå‹™å·²æ¢å¾©ï¼›æŒçºŒç›£æ§ä¸­ã€‚
- We'll publish the RCA by [date]. æˆ‘å€‘æœƒåœ¨[æ—¥æœŸ]ç™¼å¸ƒæ ¹å› åˆ†æã€‚
- Action items are tracked in the ticket. è¡Œå‹•é …å·²å»ºç«‹å·¥å–®è¿½è¹¤ã€‚
